{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91a2a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalePrice        1.000000\n",
       "OverallQual      0.790982\n",
       "GrLivArea        0.708624\n",
       "GarageCars       0.640409\n",
       "GarageArea       0.623431\n",
       "TotalBsmtSF      0.613581\n",
       "1stFlrSF         0.605852\n",
       "FullBath         0.560664\n",
       "TotRmsAbvGrd     0.533723\n",
       "YearBuilt        0.522897\n",
       "YearRemodAdd     0.507101\n",
       "GarageYrBlt      0.486362\n",
       "MasVnrArea       0.477493\n",
       "Fireplaces       0.466929\n",
       "BsmtFinSF1       0.386420\n",
       "LotFrontage      0.351799\n",
       "WoodDeckSF       0.324413\n",
       "2ndFlrSF         0.319334\n",
       "OpenPorchSF      0.315856\n",
       "HalfBath         0.284108\n",
       "LotArea          0.263843\n",
       "BsmtFullBath     0.227122\n",
       "BsmtUnfSF        0.214479\n",
       "BedroomAbvGr     0.168213\n",
       "ScreenPorch      0.111447\n",
       "PoolArea         0.092404\n",
       "MoSold           0.046432\n",
       "3SsnPorch        0.044584\n",
       "BsmtFinSF2      -0.011378\n",
       "BsmtHalfBath    -0.016844\n",
       "MiscVal         -0.021190\n",
       "Id              -0.021917\n",
       "LowQualFinSF    -0.025606\n",
       "YrSold          -0.028923\n",
       "OverallCond     -0.077856\n",
       "MSSubClass      -0.084284\n",
       "EnclosedPorch   -0.128578\n",
       "KitchenAbvGr    -0.135907\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "# We start off with the baseline import statements we need to do the basic data manipulation and visualization.\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#We create and set aside a copy of the data for initial exploration\n",
    "housing_train = pd.read_csv('../data/train.csv')\n",
    "housing = housing_train.copy()\n",
    "\n",
    "#MISSING DATA\n",
    "total = housing.isnull().sum().sort_values(ascending=False)\n",
    "percent = (housing.isnull().sum()/housing.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "#CORRELATION CHECK\n",
    "corr_matrix = housing.corr()\n",
    "top_corr = corr_matrix['SalePrice'].sort_values(ascending = False)\n",
    "\n",
    "top_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b01a5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1456 entries, 0 to 1459\n",
      "Data columns (total 66 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   MSZoning       1456 non-null   object \n",
      " 1   LotArea        1456 non-null   int64  \n",
      " 2   Street         1456 non-null   object \n",
      " 3   LotShape       1456 non-null   object \n",
      " 4   LandContour    1456 non-null   object \n",
      " 5   Utilities      1456 non-null   object \n",
      " 6   LotConfig      1456 non-null   object \n",
      " 7   LandSlope      1456 non-null   object \n",
      " 8   Neighborhood   1456 non-null   object \n",
      " 9   Condition1     1456 non-null   object \n",
      " 10  Condition2     1456 non-null   object \n",
      " 11  BldgType       1456 non-null   object \n",
      " 12  HouseStyle     1456 non-null   object \n",
      " 13  OverallQual    1456 non-null   int64  \n",
      " 14  OverallCond    1456 non-null   int64  \n",
      " 15  RoofStyle      1456 non-null   object \n",
      " 16  RoofMatl       1456 non-null   object \n",
      " 17  Exterior1st    1456 non-null   object \n",
      " 18  Exterior2nd    1456 non-null   object \n",
      " 19  MasVnrType     1448 non-null   object \n",
      " 20  MasVnrArea     1448 non-null   float64\n",
      " 21  ExterQual      1456 non-null   object \n",
      " 22  ExterCond      1456 non-null   object \n",
      " 23  Foundation     1456 non-null   object \n",
      " 24  BsmtQual       1419 non-null   object \n",
      " 25  BsmtCond       1419 non-null   object \n",
      " 26  BsmtExposure   1418 non-null   object \n",
      " 27  BsmtFinType1   1419 non-null   object \n",
      " 28  BsmtFinSF1     1456 non-null   int64  \n",
      " 29  BsmtFinType2   1418 non-null   object \n",
      " 30  BsmtUnfSF      1456 non-null   int64  \n",
      " 31  TotalBsmtSF    1456 non-null   int64  \n",
      " 32  Heating        1456 non-null   object \n",
      " 33  HeatingQC      1456 non-null   object \n",
      " 34  CentralAir     1456 non-null   object \n",
      " 35  Electrical     1455 non-null   object \n",
      " 36  2ndFlrSF       1456 non-null   int64  \n",
      " 37  LowQualFinSF   1456 non-null   int64  \n",
      " 38  GrLivArea      1456 non-null   int64  \n",
      " 39  BsmtFullBath   1456 non-null   int64  \n",
      " 40  BsmtHalfBath   1456 non-null   int64  \n",
      " 41  FullBath       1456 non-null   int64  \n",
      " 42  HalfBath       1456 non-null   int64  \n",
      " 43  BedroomAbvGr   1456 non-null   int64  \n",
      " 44  KitchenAbvGr   1456 non-null   int64  \n",
      " 45  KitchenQual    1456 non-null   object \n",
      " 46  TotRmsAbvGrd   1456 non-null   int64  \n",
      " 47  Functional     1456 non-null   object \n",
      " 48  GarageType     1375 non-null   object \n",
      " 49  GarageFinish   1375 non-null   object \n",
      " 50  GarageCars     1456 non-null   int64  \n",
      " 51  GarageQual     1375 non-null   object \n",
      " 52  GarageCond     1375 non-null   object \n",
      " 53  PavedDrive     1456 non-null   object \n",
      " 54  WoodDeckSF     1456 non-null   int64  \n",
      " 55  OpenPorchSF    1456 non-null   int64  \n",
      " 56  EnclosedPorch  1456 non-null   int64  \n",
      " 57  3SsnPorch      1456 non-null   int64  \n",
      " 58  ScreenPorch    1456 non-null   int64  \n",
      " 59  MoSold         1456 non-null   int64  \n",
      " 60  YrSold         1456 non-null   int64  \n",
      " 61  SaleType       1456 non-null   object \n",
      " 62  SaleCondition  1456 non-null   object \n",
      " 63  SalePrice      1456 non-null   int64  \n",
      " 64  Age            1456 non-null   int64  \n",
      " 65  AgeRemodel     1456 non-null   int64  \n",
      "dtypes: float64(1), int64(27), object(38)\n",
      "memory usage: 762.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#DROPPING SOME COLUMNS\n",
    "drop = ['PoolQC', 'PoolArea','MiscFeature', 'MiscVal', 'Alley', 'Fence', 'FireplaceQu', 'Fireplaces', 'LotFrontage']\n",
    "drop2 = ['Id','GarageArea','1stFlrSF','GarageYrBlt','MSSubClass','BsmtFinSF2'] \n",
    "#dropped MSSubclass and BsmtFinSF2 and RoofMatl and Exterior2nd, and Condition2 for collinearity reasons\n",
    "housing.drop(columns = drop + drop2, inplace = True)\n",
    "housing['Age'] = housing['YrSold'] - housing['YearBuilt']\n",
    "housing['AgeRemodel'] = housing['YrSold'] - housing['YearRemodAdd']\n",
    "housing = housing[housing.AgeRemodel >= 0]\n",
    "housing = housing[housing.GrLivArea < 4000]\n",
    "housing.drop(columns = ['YearBuilt','YearRemodAdd'], inplace = True)\n",
    "\n",
    "housing.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f1b8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9343071c8ae5>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  housing_numeric_one_hot['MoSold'] = housing_numeric_one_hot['MoSold'].replace({i:calendar.month_name[i][:3] for i in range(1,13)})\n",
      "C:\\Users\\abrah\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4462: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "C:\\Users\\abrah\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "#FURTHER DATA CLEANING\n",
    "housing_cat = housing.select_dtypes(exclude=[np.number])\n",
    "housing_numeric = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "#Numeric\n",
    "numeric_unbounded = ['LotArea', 'MasVnrArea','BsmtFinSF1', 'BsmtUnfSF',\n",
    "                     'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea','WoodDeckSF',\n",
    "                     'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch', 'SalePrice',\n",
    "                     'Age','AgeRemodel']\n",
    "\n",
    "#numeric_one_hot = ['MSSubClass','MoSold']\n",
    "numeric_one_hot = ['MoSold']\n",
    "numeric_ordinal = [x for x in housing_numeric.columns \n",
    "                   if (x not in numeric_unbounded and x not in numeric_one_hot)]\n",
    "\n",
    "housing_numeric_unbounded = housing_numeric[numeric_unbounded]\n",
    "#housing_numeric_unbounded['MasVnrArea'] = housing_numeric_unbounded['MasVnrArea'].fillna(0)\n",
    "housing_numeric_one_hot = housing_numeric[numeric_one_hot]\n",
    "housing_numeric_ordinal = housing_numeric[numeric_ordinal]\n",
    "\n",
    "#housing_numeric_one_hot['MSSubClass'] = housing_numeric_one_hot['MSSubClass'].astype('str')\n",
    "housing_numeric_one_hot['MoSold'] = housing_numeric_one_hot['MoSold'].replace({i:calendar.month_name[i][:3] for i in range(1,13)})\n",
    "housing_numeric_one_hot = pd.get_dummies(housing_numeric_one_hot)\n",
    "#Categorical\n",
    "cat_ordinal = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "               'BsmtFinType1', 'HeatingQC', 'KitchenQual','Functional','GarageFinish',\n",
    "               'GarageQual', 'GarageCond']\n",
    "\n",
    "housing_cat_ordinal = housing_cat[cat_ordinal]\n",
    "housing_cat_ordinal.fillna('No', inplace = True)\n",
    "housing_cat_one_hot = housing_cat.drop(columns = cat_ordinal)\n",
    "\n",
    "housing_cat_one_hot = pd.get_dummies(housing_cat_one_hot)\n",
    "#housing_cat_one_hot = housing_cat_one_hot.drop(columns = 'Exterior2nd_CBlock')\n",
    "\n",
    "def mapper(cat):\n",
    "    if cat in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n",
    "               'HeatingQC', 'KitchenQual']:\n",
    "        mapper = {'No':0, 'Po':1, 'Fa':2,'TA':3,'Gd':4,'Ex':5}\n",
    "    elif cat == 'BsmtExposure':\n",
    "            mapper = {'No':0,'No':1, 'Mn':2, 'Av':3,'Gd':4}\n",
    "    elif cat == 'BsmtFinType1':\n",
    "        mapper = {'No':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6}\n",
    "    elif cat == 'Functional':\n",
    "        mapper = {'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5, 'Min1':6,'Typ':7}\n",
    "    else:\n",
    "        mapper = {'No':0,'Unf':1,'RFn':2,'Fin':3}\n",
    "        \n",
    "    return mapper\n",
    "\n",
    "for cat in cat_ordinal:\n",
    "    housing_cat_ordinal[cat].replace(mapper(cat), inplace = True)\n",
    "\n",
    "#Combining numeric and categorical\n",
    "housing_ordinal = pd.concat([housing_numeric_ordinal,housing_cat_ordinal], axis = 'columns')\n",
    "housing_one_hot = pd.concat([housing_numeric_one_hot, housing_cat_one_hot], axis = 'columns')\n",
    "housing_clean = pd.concat([housing_one_hot, housing_ordinal, housing_numeric_unbounded], \n",
    "                          axis = 'columns')\n",
    "#Getting rid of skew\n",
    "skewed_feats = housing_clean[numeric_unbounded].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "housing_clean[skewed_feats] = np.log1p(housing_clean[skewed_feats])\n",
    "\n",
    "#MORE CORRELATION\n",
    "ordinal_prices = pd.concat([housing_ordinal, housing['SalePrice']], axis = 'columns')\n",
    "ordinal_corr_matrix = ordinal_prices.corr()\n",
    "top_corr_ordinal = ordinal_corr_matrix['SalePrice'].sort_values(ascending = False)\n",
    "\n",
    "one_hot_prices = pd.concat([housing_one_hot, housing['SalePrice']], axis = 'columns')\n",
    "one_hot_corr_matrix = one_hot_prices.corr()\n",
    "top_corr_one_hot = one_hot_corr_matrix['SalePrice'].filter(like = 'Neighborhood').sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d3650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoSold_Apr          0\n",
       "Foundation_Slab     0\n",
       "Foundation_Wood     0\n",
       "BsmtFinType2_ALQ    0\n",
       "BsmtFinType2_BLQ    0\n",
       "                   ..\n",
       "BldgType_1Fam       0\n",
       "BldgType_2fmCon     0\n",
       "BldgType_Duplex     0\n",
       "BldgType_Twnhs      0\n",
       "AgeRemodel          0\n",
       "Length: 228, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REMOVAL OF THE REMAINING NaN\n",
    "housing_clean.isnull().sum().sort_values(ascending=False)\n",
    "df = housing_clean.copy()\n",
    "problem_col = df.isin([np.nan, np.inf, -np.inf]).sum(axis=0)[df.isin([np.nan, np.inf, -np.inf]).sum(axis=0) != 0] \n",
    "index_to_drop = df[problem_col.index[0]][df[problem_col.index[0]].isin([np.nan, np.inf, -np.inf])].index\n",
    "df.drop(index = index_to_drop, inplace = True)\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08a6b6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3e51f952967f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mforest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mforest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    181\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    182\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST FOR FEATURE IMPORTANCE\n",
    "X_train = df.drop(columns = ['SalePrice'])\n",
    "y_train = df['SalePrice']\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500, max_depth=4)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "forest.feature_importances_\n",
    "score_df = pd.DataFrame({'feature':X_train.columns,\n",
    "                            'importance_score': forest.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8461b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will look at feature importances and their correlation with the 'SalePrice'\n",
    "score_df.sort_values('feature', inplace=True)\n",
    "top_corr = df.corr()['SalePrice'].abs().drop(index = ['SalePrice']) #I suppose we want to look at the absolute value\n",
    "                                                                    #of the correlation. Is that right?\n",
    "top_corr.sort_index(inplace=True) \n",
    "#now rows of score_df and top_corr match and we can add the values of correlation\n",
    "score_df['correlation'] = top_corr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ede219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_df.sort_values(ascending=False, by = ['importance_score'], inplace = True)\n",
    "score_df['importance_score_rank'] = [k for k in range(1,1+len(score_df.index))]\n",
    "score_df.sort_values(ascending=False, by = ['correlation'], inplace = True)\n",
    "score_df['correlation_rank'] = [k for k in range(1,1+len(score_df.index))]\n",
    "score_df['overall_rank'] = (score_df['importance_score_rank'] + score_df['correlation_rank'])/2 \n",
    "score_df.sort_values(ascending=True, by = ['overall_rank'], inplace = True)\n",
    "score_df.reset_index(drop = True, inplace = True)\n",
    "score_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e60ce",
   "metadata": {},
   "source": [
    "Correlation measures only linear dependence between variables and it does not detect non-linear dependence (in particular cor(X,Y) can be 0 for random variables X and Y=X^2, which are of course completely dependent). So, if a given feature has high feature_importance score, but low correlation it means that 'SalePrice' depend on it in a non-linear manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45386661",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(score_df['importance_score_rank'][:20], score_df['correlation_rank'][:20], c =\"blue\",\n",
    "            linewidths = 1)\n",
    "plt.title('importance_rank vs correlation_rank')\n",
    "plt.xticks(np.arange(0, 40, step=1))\n",
    "plt.yticks(np.arange(0, 40, step=1))\n",
    "plt.xlabel(\"importance_score_rank\")\n",
    "plt.ylabel(\"correlation_rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e1096",
   "metadata": {},
   "source": [
    "## Using Linear Regression models to predict housing prices\n",
    "Below, we compare several candidate models to predict the housing prices. We will compare the baselined model of sale price average along with ordinary least squares, ridge regression, lasso regression, and elastic net regression. We will be operating with log housing prices to combat heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ba585",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Regression Coefficients blow up due to collinearity in categorical data\n",
    "One weird thing is that the ordinary Least Squares Regression Coefficients are on radically different scales from one another. We see very negative coefficients along with very large positive coefficients. We make some plots of the scales of the regression coefficients below. We notice that the scales of the categorical variables seem to be much greater than those of the unbounded or the ordinal variables. Ordinary Least Squares is ill fit to deal with the multicollinearity of these categorical variables. This is taken care of via regularization which limits the size of the coefficients. We'll describe regularization later. OLS blowup can be mitigated by dummy encoding the variables instead of one-hot but I was still having collinearity problems on cross validation for the dummy encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf54fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We scale all the data note X_train and y_train are fairly clean\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns = 'SalePrice')\n",
    "y = (df['SalePrice'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 20)\n",
    "\n",
    "#We call the standard Scaler object and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "\n",
    "#We work with the scaled training set and log prices\n",
    "X_train = (scaler.transform(X_train))\n",
    "X_test = (scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acfdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our baseline model looking at the average\n",
    "y_pred_average = np.ones(len(y_test)) * np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea166583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our Simple Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "slr = LinearRegression(copy_X = True)\n",
    "slr.fit(X_train, y_train)\n",
    "y_pred_slr = (slr.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison of the RMSE. It is huge on the new data but relatively good on the training set suggesting overfit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred_average, squared = False))\n",
    "print(mean_squared_error(y_train, (slr.predict(X_train)), squared = False))\n",
    "\n",
    "print(mean_squared_error((y_test), (y_pred_slr), squared = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ad36c",
   "metadata": {},
   "source": [
    "Here we record some of the blow up of the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe showing which coefficients correspond\n",
    "print('Linear Regression Coefficients')\n",
    "slr_coefficients = pd.DataFrame(slr.coef_.reshape(1,-1), columns = X.columns)\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "print_full(slr_coefficients.T[0].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plot of the scales of the linear regression coefficients.\n",
    "log_slr_coef = np.log10(np.abs(slr.coef_))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data = log_slr_coef)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830991af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots of the unbounded and ordinal variable coefficient scales\n",
    "numeric_train_unbounded = [x for x in numeric_unbounded if x != 'SalePrice']\n",
    "log_coefs = np.log10(np.abs(slr_coefficients))\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data = log_coefs[numeric_train_unbounded])\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data = log_coefs[numeric_ordinal + cat_ordinal])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb1370",
   "metadata": {},
   "source": [
    "# Ordinary least squares on the numerical data\n",
    "Instead we look at an OLS baseline for the numerical data, excluding the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33049ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'SalePrice')\n",
    "y = df['SalePrice']\n",
    "numeric_train_unbounded = [x for x in numeric_unbounded if x!= 'SalePrice']\n",
    "numeric_train = numeric_train_unbounded + numeric_ordinal + cat_ordinal\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 20)\n",
    "\n",
    "#We call the standard Scaler object and fit it\n",
    "X_train = X_train[numeric_train]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "\n",
    "slr = LinearRegression(copy_X = True)\n",
    "slr.fit(X_train, y_train)\n",
    "y_pred_slr = (slr.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1e179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slr_coefficients = pd.DataFrame(slr.coef_.reshape(1,-1), columns = X_train.columns)\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "print_full(slr_coefficients.T[0].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef418639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plot of the scales of the linear regression coefficients.\n",
    "log_slr_coef = np.log10(np.abs(slr.coef_))\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data = log_slr_coef)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "residuals = y_train - y_pred_slr\n",
    "plt.scatter(y_pred_slr, residuals)\n",
    "\n",
    "\n",
    "plt.xlabel(\"predictions\", fontsize=16)\n",
    "plt.ylabel(\"residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea589df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'SalePrice')\n",
    "y = (df['SalePrice'])\n",
    "numeric_train_unbounded = [x for x in numeric_unbounded if x!= 'SalePrice']\n",
    "numeric_train = numeric_train_unbounded + numeric_ordinal + cat_ordinal\n",
    "X = X[numeric_train]\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(5, shuffle = True, random_state = 69)\n",
    "\n",
    "mses = np.zeros((2,5))\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    x_t = X.iloc[train_index]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_t)\n",
    "    x_t_scale = scaler.transform(x_t)\n",
    "    y_t = y.iloc[train_index]\n",
    "    \n",
    "    x_ho = X.iloc[test_index]\n",
    "    x_ho_scale = scaler.transform(x_ho)\n",
    "    y_ho = y.iloc[test_index]\n",
    "    \n",
    "    pred0 = y_t.mean() * np.ones(len(x_ho))\n",
    "    \n",
    "    \n",
    "    model = LinearRegression(copy_X = True)\n",
    "    model.fit(x_t_scale,y_t)\n",
    "    \n",
    "    pred1 = (model.predict(x_ho_scale))\n",
    "    \n",
    "    mses[0,i] = mean_squared_error(np.exp(y_ho),np.exp(pred0), squared = False)\n",
    "    \n",
    "    mses[1,i] = mean_squared_error(np.exp(y_ho),np.exp(pred1), squared = False)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.scatter(np.zeros(5), \n",
    "            mses[0,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black',\n",
    "            label=\"Single Split\")\n",
    "plt.scatter(np.ones(5), \n",
    "            mses[1,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black')\n",
    "\n",
    "plt.scatter([0,1], np.mean(mses, axis=1), s=60, c='r', label=\"Mean\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xticks([0,1],[\"Baseline\", \"OLS\"], fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Model\", fontsize=16)\n",
    "plt.ylabel(\"RMSE\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "(np.mean(mses[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f335d",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "The OLS coefficient blowup is due to the artifically high model complexity and the multicollinearity indtroduced by the one hot encoding of the categorical variables. We can keep them in check by constraining them with lasso,ridge and elastic net regularization models and seeing how they control the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77749df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use gridsearchCV to fit a Ridge Regression model to the data\n",
    "X = df.drop(columns = 'SalePrice')\n",
    "y = (df['SalePrice'])\n",
    "\n",
    "#Import Statements\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "ridge_pipe = Pipeline([('scale',RobustScaler()),\n",
    "                              ('ridge',Ridge())])\n",
    "\n",
    "\n",
    "#Grid Search CV\n",
    "parameters = {'ridge__alpha': np.logspace(0, 4, 10)}\n",
    "ridge_cv = GridSearchCV(ridge_pipe,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "ridge_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cead7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7b84e",
   "metadata": {},
   "source": [
    "# Lasso Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'SalePrice')\n",
    "y = (df['SalePrice'])\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_pipe = Pipeline([('scale',RobustScaler()),\n",
    "                              ('lasso',Lasso())])\n",
    "\n",
    "parameters = {'lasso__alpha': np.logspace(-3, -1, 10)}\n",
    "lasso_cv = GridSearchCV(lasso_pipe,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "lasso_cv.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69df95",
   "metadata": {},
   "source": [
    "# Comparison of the Ridge and Lasso Estimators\n",
    "Here we look at the coefficients of the best Ridge and Lasso estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_estimator = ridge_cv.best_estimator_\n",
    "lasso_estimator = lasso_cv.best_estimator_\n",
    "\n",
    "ridge_coefs = ridge_estimator['ridge'].coef_\n",
    "lasso_coefs = lasso_estimator['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c158f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coefs = pd.DataFrame(ridge_coefs.reshape(1,-1), columns = X.columns)\n",
    "lasso_coefs= pd.DataFrame(lasso_coefs.reshape(1,-1), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients of the Ridge Regression\n",
    "print_full(ridge_coefs.T[0].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8feea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients of the Lasso Regression\n",
    "print_full(lasso_coefs[lasso_coefs != 0].T[0].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ridge_estimator.predict(X)\n",
    "residuals = y - y_hat\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(y_hat, residuals)\n",
    "\n",
    "\n",
    "plt.xlabel(\"predictions\", fontsize=16)\n",
    "plt.ylabel(\"residuals\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(y, y_hat)\n",
    "\n",
    "\n",
    "plt.xlabel(\"y\", fontsize=16)\n",
    "plt.ylabel(\"y_hat\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e05fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat = lasso_estimator.predict(X)\n",
    "residuals = y - y_hat\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(y_hat, residuals)\n",
    "plt.xlabel(\"predictions\", fontsize=16)\n",
    "plt.ylabel(\"residuals\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(y, y_hat)\n",
    "\n",
    "\n",
    "plt.xlabel(\"y\", fontsize=16)\n",
    "plt.ylabel(\"y_hat\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88740e",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "A mixture of both the lasso and the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'SalePrice')\n",
    "y = (df['SalePrice'])\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_pipe = Pipeline([('scale',RobustScaler()),\n",
    "                              ('elastic',ElasticNet())])\n",
    "parameters = {'elastic__alpha': np.logspace(-3, 0, 10), 'elastic__l1_ratio': [.1, .5, .7, .9, .95, .99,\n",
    "1]}\n",
    "\n",
    "elastic_cv = GridSearchCV(elastic_pipe,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "elastic_cv.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_estimator = elastic_cv.best_estimator_\n",
    "elastic_coefs = elastic_estimator['elastic'].coef_\n",
    "elastic_coefs = pd.DataFrame(elastic_coefs.reshape(1,-1), columns = X.columns)\n",
    "print_full(elastic_coefs[elastic_coefs != 0].T[0].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fa790",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = elastic_estimator.predict(X)\n",
    "residuals = y - y_hat\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(y_hat, residuals)\n",
    "\n",
    "\n",
    "plt.xlabel(\"predictions\", fontsize=16)\n",
    "plt.ylabel(\"residuals\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(y, y_hat)\n",
    "\n",
    "\n",
    "plt.xlabel(\"y\", fontsize=16)\n",
    "plt.ylabel(\"y_hat\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c60bb9",
   "metadata": {},
   "source": [
    "## Using ElasticNet for feature selection\n",
    "Running ElasticNet on the dataset with zero'd features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016708c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero = elastic_coefs.loc[:, (elastic_coefs != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[non_zero.columns]\n",
    "y = (df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21686daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_pipe = Pipeline([('scale',RobustScaler()),\n",
    "                              ('elastic',ElasticNet())])\n",
    "parameters = {'elastic__alpha': np.logspace(-3, 0, 10), 'elastic__l1_ratio': [.1, .5, .7, .9, .95, .99,\n",
    "1]}\n",
    "\n",
    "elastic_cv = GridSearchCV(elastic_pipe,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "elastic_cv.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elastic_cv.best_score_)\n",
    "elastic_estimator = elastic_cv.best_estimator_\n",
    "elastic_coefs = elastic_estimator['elastic'].coef_\n",
    "elastic_coefs = pd.DataFrame(elastic_coefs.reshape(1,-1), columns = X.columns)\n",
    "print_full(elastic_coefs[elastic_coefs != 0].T[0].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = elastic_estimator.predict(X)\n",
    "print(mean_squared_error(np.exp(y),np.exp(y_hat), squared = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21483ee",
   "metadata": {},
   "source": [
    "# Further Ideas/Comments\n",
    "- Introduce interaction terms between the top k most important features which are numeric and categorical after elastic net selection. (Regression after elastic net selection seems to do a good job?).\n",
    "- For some reason after I introduced the skew the forest stopped working?\n",
    "- Check out the boosted tree models, and ensemble them with the linear regression models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
